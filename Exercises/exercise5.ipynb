{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) By defining the random variable of X as uniformly distribute between 0 and 1, the MC integration can be computed by generating n samples of X and computing the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1555,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated integral: 1.7144059665442293, Variance: 0.24756229917930767\n",
      "Reference: 1.718281828459045\n",
      "95% Confidence interval: [0.7391955643605629, 2.689616368727896]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy.stats as st\n",
    "\n",
    "def conf_int (mean, std, n):\n",
    "       Z = st.t.ppf(0.95, n)\n",
    "       return (mean - Z * (std/math.sqrt(n)), mean + Z * (std/math.sqrt(n)))\n",
    "\n",
    "n_samples = 100\n",
    "\n",
    "def monte_carlo_integration(n_samples):\n",
    "    # Generate random samples uniformly distributed in [0, 1]\n",
    "    samples = np.random.uniform(0, 1, n_samples)\n",
    "\n",
    "    # Evaluate the function e^x at each sample point\n",
    "    function_values = np.exp(samples)\n",
    "    estimate = np.mean(function_values)\n",
    "    variance =np.var(function_values)\n",
    "\n",
    "    return estimate, variance\n",
    "\n",
    "estimate, variance = monte_carlo_integration(n_samples)\n",
    "print(f\"Estimated integral: {estimate}, Variance: {variance}\")\n",
    "print(f\"Reference: {np.exp(1) - 1}\")\n",
    "\n",
    "# 95% confidence interval\n",
    "confidence_level = 0.95\n",
    "alpha = 1 - confidence_level\n",
    "z = 1.96  # Standard normal deviate corresponding to the given confidence level\n",
    "\n",
    "confidence_interval = (estimate - z * np.sqrt(variance), estimate + z * np.sqrt(variance))\n",
    "print(f\"95% Confidence interval: [{confidence_interval[0]}, {confidence_interval[1]}]\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the confidence interval is quite large for this approximation (high variance).\n",
    "The following methods are used to reduce applied to reduce this variance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) For the estimation of the integral using antithetic variable, we compute the antithetic path for every rv of an uniformly distributed sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated integral: 1.7244375159569298, Variance: 0.0037943188217164463\n",
      "Reference: 1.718281828459045\n",
      "95% Confidence interval: [1.603705352945138, 1.8451696789687217]\n"
     ]
    }
   ],
   "source": [
    "def antithetic_integration(n_samples):\n",
    "    # Generate random samples uniformly distributed in [0, 1]\n",
    "    samples = np.random.uniform(0, 1, n_samples)\n",
    "\n",
    "    # for every sample path, take its antithetic path and average the function values\n",
    "    \n",
    "    function_samples =( np.exp(samples) + np.exp(1 - samples) ) / 2\n",
    "\n",
    "    estimate = np.mean(function_samples)\n",
    "    variance =np.var(function_samples)\n",
    "\n",
    "    return estimate, variance\n",
    "\n",
    "\n",
    "\n",
    "estimate, variance = antithetic_integration(n_samples)\n",
    "print(f\"Estimated integral: {estimate}, Variance: {variance}\")\n",
    "print(f\"Reference: {np.exp(1) - 1}\")\n",
    "\n",
    "#95% confidence interval\n",
    "confidence_level = 0.95\n",
    "alpha = 1 - confidence_level\n",
    "z = 1.96  # Standard normal deviate corresponding to the given confidence level\n",
    "\n",
    "confidence_interval = (estimate - z * np.sqrt(variance), estimate + z * np.sqrt(variance))\n",
    "print(f\"95% Confidence interval: [{confidence_interval[0]}, {confidence_interval[1]}]\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the variance was significantly reduced when compared to the MC integration method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) We compute the value of a control variable, c, by computing the covariance between a uniformly distributed sample Y and an exponentially distributed sample derived X from Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated integral: 1.710714718263442, Variance: 0.00363513696385033\n",
      "Reference: 1.718281828459045\n",
      "95% Confidence interval: [1.5925422080915435, 1.8288872284353404]\n"
     ]
    }
   ],
   "source": [
    "def control_variates(n_samples):\n",
    "    Y = np.random.uniform(0, 1, n_samples)\n",
    "    X = np.exp(Y)\n",
    "\n",
    "    cov1 = np.mean(Y*X) - np.mean(Y) * np.mean(X)\n",
    "    var1 = np.var(Y)\n",
    "\n",
    "    c = -cov1 / var1\n",
    "\n",
    "\n",
    "    Z = X + c*(Y - np.mean(Y))\n",
    "    \n",
    "    estimate = np.mean(Z)\n",
    "    variance =np.var(Z)\n",
    "\n",
    "    return estimate, variance\n",
    "\n",
    "\n",
    "estimate, variance = control_variates(n_samples)\n",
    "print(f\"Estimated integral: {estimate}, Variance: {variance}\")\n",
    "print(f\"Reference: {np.exp(1) - 1}\")\n",
    "\n",
    "#95% confidence interval\n",
    "confidence_level = 0.95\n",
    "alpha = 1 - confidence_level\n",
    "z = 1.96  # Standard normal deviate corresponding to the given confidence level\n",
    "\n",
    "confidence_interval = (estimate - z * np.sqrt(variance), estimate + z * np.sqrt(variance))\n",
    "print(f\"95% Confidence interval: [{confidence_interval[0]}, {confidence_interval[1]}]\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance appears to have reduced slightly compared to the antithetic variable integration method, but it's not significant."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) For the stratified sampling method, we sample in specific areas based on what we know about the sampling space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated integral: 1.7200744813492446, Variance: 0.0025698683075879723\n",
      "Reference: 1.718281828459045\n",
      "95% Confidence interval: [1.620714499010126, 1.819434463688363]\n"
     ]
    }
   ],
   "source": [
    "def stratified_sampling(n_samples):\n",
    "    U = np.random.uniform(0, 1, n_samples)\n",
    "    W = []\n",
    "\n",
    "    #compute the stratified sample for each rv in u\n",
    "    for u in U:\n",
    "        sum = 0\n",
    "        for j in range(10):\n",
    "            sum += np.exp(j/10 + u/10 )\n",
    "        #append the result to the final sample\n",
    "        W.append(sum / 10)\n",
    "\n",
    "\n",
    "    \n",
    "    estimate = np.mean(W)\n",
    "    variance =np.var(W)\n",
    "\n",
    "    return estimate, variance\n",
    "\n",
    "estimate, variance = stratified_sampling(n_samples)\n",
    "print(f\"Estimated integral: {estimate}, Variance: {variance}\")\n",
    "print(f\"Reference: {np.exp(1) - 1}\")\n",
    "\n",
    "#95% confidence interval\n",
    "confidence_level = 0.95\n",
    "alpha = 1 - confidence_level\n",
    "z = 1.96  # Standard normal deviate corresponding to the given confidence level\n",
    "\n",
    "confidence_interval = (estimate - z * np.sqrt(variance), estimate + z * np.sqrt(variance))\n",
    "print(f\"95% Confidence interval: [{confidence_interval[0]}, {confidence_interval[1]}]\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance was further reduced in comparison to the control variate integration method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) We are able to reduce the variance of the estimate of the fraction of blocked customers by finding the control variate, obtained from the covariance between the sample containing the franction of blocked customers and the interarrival times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate1: 0.11225999999999997, Variance1: 1.439999999999982e-08\n",
      "95% Confidence interval: [0.11202479999999997, 0.11249519999999998]\n"
     ]
    }
   ],
   "source": [
    "customers = 10000\n",
    "k = 2.05\n",
    "sim = 10\n",
    "\n",
    "\n",
    "\n",
    "l=1\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "m = 10\n",
    "s = 8.0\n",
    "p_dist = np.random.exponential(1, size=customers)\n",
    "exp_dist = np.random.exponential(8, customers)\n",
    "\n",
    "    \n",
    "\n",
    "def blocked_customers (inter_arrival_dist, service_times_dist, m, s, customers, sim):\n",
    "    fract_blocked_counts = []\n",
    "    blocked_customer_count = 0\n",
    "\n",
    "    #set service time for all units to zero\n",
    "    service_unit_time = np.zeros(m)\n",
    "\n",
    "    for _ in range(sim):\n",
    "        blocked_customer_count = 0\n",
    "        #go through all customers\n",
    "        for i in range(customers):\n",
    "            #generate random arrival time for coming customer\n",
    "            arrival_time = inter_arrival_dist[i]\n",
    "            #subtract the arrival time from the service units times\n",
    "            service_unit_time -= arrival_time\n",
    "\n",
    "            #sort the service units with increasing time\n",
    "            service_unit_time = np.sort(np.maximum(service_unit_time, 0.0))\n",
    "\n",
    "            #check if there are service units \n",
    "            if (len([i for i in service_unit_time if i < 0.1]) == 0):\n",
    "                blocked_customer_count += 1\n",
    "            else:\n",
    "                service_unit_time[0] = service_times_dist[i]\n",
    "                    \n",
    "        fract_blocked_counts.append(blocked_customer_count/customers)\n",
    "    \n",
    "    return fract_blocked_counts\n",
    "\n",
    "\n",
    "customers = 10000\n",
    "x_m = 8 * (2.05 - 1) / 2.05\n",
    "l = 1\n",
    "\n",
    "# Compute sample X, containing the fractions of blocked customers\n",
    "X = blocked_customers (p_dist, exp_dist, 10, 8, customers, 10)\n",
    "\n",
    "# Y is the sample for the interarrival times\n",
    "Y = p_dist\n",
    "\n",
    "new_estimates = []\n",
    "\n",
    "# go through each fraction of blocked customers\n",
    "for x in X:\n",
    "\n",
    "    cov1 = np.mean(Y*x) - np.mean(Y) * x\n",
    "    var1 = np.var(Y)\n",
    "\n",
    "    #Compute the control variable\n",
    "    c = -cov1 / var1\n",
    "\n",
    "    #Compute the new estimate\n",
    "    Z = X + c*(np.mean(Y) - 1/l)\n",
    "\n",
    "    #Append new estimate to the final sample\n",
    "    new_estimates.append(Z)\n",
    "\n",
    "estimate = np.mean(new_estimates)\n",
    "variance =np.var(new_estimates)\n",
    "\n",
    "print(f\"Estimate1: {estimate}, Variance1: {variance}\")\n",
    "\n",
    "#95% confidence interval\n",
    "confidence_level = 0.95\n",
    "alpha = 1 - confidence_level\n",
    "z = 1.96  # Standard normal deviate corresponding to the given confidence level\n",
    "\n",
    "confidence_interval = (estimate - z * np.sqrt(variance), estimate + z * np.sqrt(variance))\n",
    "print(f\"95% Confidence interval: [{confidence_interval[0]}, {confidence_interval[1]}]\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance of the estimate is significantly reduced."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) We compute the sample containing the fractions of blocked customers from 20 simulations, by deriving the hyperexponentially distributed sample and the exponentially distributed sample from the same uniformly distributed sample of random varuiables, U. We then perform a 2-samples test statistic to verify if there is an effect between the means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1547,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = 10000\n",
    "sim = 20\n",
    "m = 10\n",
    "s = 8\n",
    "mean_tbc = 1\n",
    "arrival_intensity = 1\n",
    "\n",
    "u = np.random.uniform(0, 1, customers)\n",
    "\n",
    "# Reprogramming the return value of the function and the interarrival distributions\n",
    "def blocking_system_collect (inter_arrival_dist, service_times_dist, m, s, customers, sim):\n",
    "  fract_blocked_counts = []\n",
    "  blocked_customer_count = 0\n",
    "\n",
    "  #set service time for all units to zero\n",
    "  service_unit_time = np.zeros(m)\n",
    "\n",
    "  for _ in range(sim):\n",
    "    blocked_customer_count = 0\n",
    "    #go through all customers\n",
    "    for i in range(customers):\n",
    "      #generate random arrival time for coming customer\n",
    "      if (inter_arrival_dist == \"hyper-exponential\"):\n",
    "        e = 0.8 * (-np.log(u)/0.8333) + 0.2 * (-np.log(u)/5)\n",
    "        # EXERCISE 5, QUESTION 6 -> set arrival time to ith randomly generated hyperexponential dist. arrival time\n",
    "        arrival_time =  e[i]\n",
    "      elif (inter_arrival_dist == \"erlang\") :\n",
    "        arrival_time = st.erlang.rvs(1, size=1, scale=mean_tbc)[0]\n",
    "      else:\n",
    "         # EXERCISE 5, QUESTION 6 -> set arrival time to ith randomly generated Poisson dist. arrival time\n",
    "         arrival_time =  (-np.log(u)/l) [i]\n",
    "\n",
    "\n",
    "      #subtract the arrival time from the service units times\n",
    "      service_unit_time -= arrival_time\n",
    "\n",
    "      #sort the service units with increasing time\n",
    "      service_unit_time = np.sort(np.maximum(service_unit_time, 0.0))\n",
    "\n",
    "      #check if there are service units \n",
    "      if (len([i for i in service_unit_time if i < 0.1]) == 0):\n",
    "        blocked_customer_count += 1\n",
    "      else:\n",
    "        if (service_times_dist == \"exponential\"):\n",
    "          service_unit_time[0] = np.random.exponential(8, 1)[0]\n",
    "        elif (service_times_dist == \"constant\"):\n",
    "          service_unit_time[0] = s\n",
    "        elif (service_times_dist == \"pareto_1.05\"):\n",
    "          x_m = s * (1.05 - 1) / 1.05\n",
    "          service_unit_time[0] = (( np.random.pareto(1.05, size=1) + 1) * x_m ) [0]\n",
    "        elif (service_times_dist == \"pareto_2.05\"):\n",
    "          x_m = s * (2.05 - 1) / 2.05\n",
    "          service_unit_time[0] = (( np.random.pareto(2.05, size=1) + 1) * x_m ) [0]\n",
    "        elif (service_times_dist == \"chi-squared\"):\n",
    "          service_unit_time[0] = np.random.chisquare(s, 1)[0]\n",
    "        else:\n",
    "           service_unit_time[0] = np.random.exponential(8, 1)[0]\n",
    "\n",
    "        #service_unit_time[0] = service_times_dist[i]\n",
    "                \n",
    "    fract_blocked_counts.append(blocked_customer_count/customers)\n",
    "\n",
    "  return fract_blocked_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value = 0.28945047926492645\n",
      "Accept null hypothesis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4l/6stdwbh56pvgwz8thwnlgtq80000gn/T/ipykernel_40213/2982088959.py:18: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  result = st.ttest_ind(a=blocked_pois, b=blocked_hyper, equal_var=False)\n"
     ]
    }
   ],
   "source": [
    "customers = 10000\n",
    "l = 1\n",
    "\n",
    "u = np.random.uniform(0, 1, customers)\n",
    "\n",
    "#Poisson interarrival times based on inverse transform sampling\n",
    "p_dist = -np.log(u)/l\n",
    "\n",
    "# Hyperexponential interarrival times based on inverse transform sampling\n",
    "hyperexp = 0.8 * (-np.log(u)/0.8333) + 0.2 * (-np.log(u)/5)\n",
    "\n",
    "p_dist1 = np.random.exponential(1, size=customers)\n",
    "\n",
    "blocked_pois = blocking_system_collect (\"exponential\", \"exponential\", m, s, customers, sim)\n",
    "blocked_hyper = blocking_system_collect (\"hyperexponential\", \"constant\", m, s, customers, sim)\n",
    "\n",
    "\n",
    "result = st.ttest_ind(a=blocked_pois, b=blocked_hyper, equal_var=False)\n",
    "\n",
    "print(f\"p-value = {result.pvalue}\")\n",
    "if (result.pvalue < 0.05):\n",
    "  print(\"Reject null hypothesis\")\n",
    "else:\n",
    "    print(\"Accept null hypothesis\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the p-value is greater than the significance level of 0.05, there is enough evidence to accept the null hypothesis, that the means between the two samples are equal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) We use the Crude MC estimator to find the probability that a standard normal random variable Z is greater than a. Given a standard normal sample of random variables, we traverse through the array and count the number of variables that satisfy the condition and divide it by the size of the sample. We repeat this 100 times and compute the average and variance of the probabilities collected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance = 1\n",
      "a = 2\n",
      "  n = 1000: Estimate: 0.023899999999999998, Variance: 0.00021178999999999998 \n",
      "  n = 10000: Estimate: 0.023899999999999998, Variance: 0.00027779 \n",
      "  n = 100000: Estimate: 0.024199999999999996, Variance: 0.00027236 \n",
      "variance = 1\n",
      "a = 4\n",
      "  n = 1000: Estimate: 0.0, Variance: 0.0 \n",
      "  n = 10000: Estimate: 0.0, Variance: 0.0 \n",
      "  n = 100000: Estimate: 0.0001, Variance: 9.9e-07 \n",
      "variance = 2\n",
      "a = 2\n",
      "  n = 1000: Estimate: 0.021, Variance: 0.000235 \n",
      "  n = 10000: Estimate: 0.023599999999999996, Variance: 0.00024704 \n",
      "  n = 100000: Estimate: 0.023, Variance: 0.000269 \n",
      "variance = 2\n",
      "a = 4\n",
      "  n = 1000: Estimate: 0.0, Variance: 0.0 \n",
      "  n = 10000: Estimate: 0.0001, Variance: 9.899999999999998e-07 \n",
      "  n = 100000: Estimate: 0.0001, Variance: 9.899999999999994e-07 \n",
      "variance = 3\n",
      "a = 2\n",
      "  n = 1000: Estimate: 0.026099999999999995, Variance: 0.00022778999999999993 \n",
      "  n = 10000: Estimate: 0.0264, Variance: 0.00026304 \n",
      "  n = 100000: Estimate: 0.022899999999999997, Variance: 0.00018259 \n",
      "variance = 3\n",
      "a = 4\n",
      "  n = 1000: Estimate: 0.0, Variance: 0.0 \n",
      "  n = 10000: Estimate: 0.0, Variance: 0.0 \n",
      "  n = 100000: Estimate: 0.0, Variance: 0.0 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = 2\n",
    "\n",
    "n_samples = 100\n",
    "\n",
    "\n",
    "def monte_carlo_estimator(n_samples, a, sigma):\n",
    "\n",
    "    prob_collector = []\n",
    "    for _ in range(100):\n",
    "        samples = np.random.normal(0, 1, n_samples)\n",
    "        # Check \n",
    "        greater_a = [x for x in samples if x > a]\n",
    "        probs = len(greater_a) / n_samples\n",
    "        prob_collector.append(probs)\n",
    "        \n",
    "\n",
    "    probability = np.mean(prob_collector)\n",
    "    variance = np.var(prob_collector)\n",
    "\n",
    "    return probability, variance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a_list = [2, 4]\n",
    "sample_sizes = [10**3, 10**4, 10**5]\n",
    "vars = [1, 2, 3]\n",
    "\n",
    "# Print loop\n",
    "for v in vars:\n",
    "    for a in a_list:\n",
    "        print(f\"variance = {v}\")\n",
    "        print(f\"a = {a}\")\n",
    "        for n in sample_sizes:\n",
    "            estimate, variance = monte_carlo_estimator(n_samples, a, np.sqrt(v))\n",
    "            print(f\"  n = {n}: Estimate: {estimate}, Variance: {variance} \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) For Importance Sampling, we define f(x) as the pdf of the standard normal distribution, the proposed distribution g(x) as the pdf of a normal distribution with mean a and standard deviation equal to the square root of a given variance. h(x) is defined as the indicator function that given a sample, replaces every variable by 1 if it is greater than a or by 0 if it isn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance = 1\n",
      "  a = 2\n",
      "    n = 1000: Estimate: 0.021999896129236543, Variance: 0.0011944171169602793 \n",
      "    n = 10000: Estimate: 0.022981442467991737, Variance: 0.0012131586489093392 \n",
      "    n = 100000: Estimate: 0.022895554328816556, Variance: 0.0012126650112229368 \n",
      "Variance = 1\n",
      "  a = 4\n",
      "    n = 1000: Estimate: 3.224950666363084e-05, Variance: 4.7506189184903115e-09 \n",
      "    n = 10000: Estimate: 3.070437976489888e-05, Variance: 4.333783473247412e-09 \n",
      "    n = 100000: Estimate: 3.1615550776086144e-05, Variance: 4.5169079020710746e-09 \n",
      "Variance = 2\n",
      "  a = 2\n",
      "    n = 1000: Estimate: 0.02398779898241075, Variance: 0.0018005557710144174 \n",
      "    n = 10000: Estimate: 0.024886263265550407, Variance: 0.0019744440265507556 \n",
      "    n = 100000: Estimate: 0.024293774489027656, Variance: 0.0018974125899436378 \n",
      "Variance = 2\n",
      "  a = 4\n",
      "    n = 1000: Estimate: 2.4467733719319366e-05, Variance: 4.253424720398603e-09 \n",
      "    n = 10000: Estimate: 3.0314460939562914e-05, Variance: 6.3812967099528585e-09 \n",
      "    n = 100000: Estimate: 3.318124638476037e-05, Variance: 6.9858297436122185e-09 \n",
      "Variance = 3\n",
      "  a = 2\n",
      "    n = 1000: Estimate: 0.02530932424611334, Variance: 0.002518517408613433 \n",
      "    n = 10000: Estimate: 0.023965300108736848, Variance: 0.002363068121490548 \n",
      "    n = 100000: Estimate: 0.02497570480635304, Variance: 0.0024510800765434543 \n",
      "Variance = 3\n",
      "  a = 4\n",
      "    n = 1000: Estimate: 3.354836488196123e-05, Variance: 8.652745821593905e-09 \n",
      "    n = 10000: Estimate: 3.30815811144231e-05, Variance: 8.855069574256175e-09 \n",
      "    n = 100000: Estimate: 3.286095484901603e-05, Variance: 8.66116354995351e-09 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def importance_sampling(a, n_samples, var1):\n",
    "\n",
    "    # Compute sample average of I(Z > a)\n",
    "    indicator = lambda l: 1 if l > a else 0\n",
    "\n",
    "    # cdf of normal distribution (0, 1)\n",
    "    def f(x) : return np.exp(-0.5*(x-0)**2)/ (1 * np.sqrt(2*np.pi))\n",
    "\n",
    "    #cdf of normal distribution (a, var1)\n",
    "    def g (x, a, o): return np.exp(-0.5*(x-a)**2)/ (o * np.sqrt(2*np.pi))\n",
    "\n",
    "    def h(x) : return [indicator(i) for i in x]\n",
    "    \n",
    "    \n",
    "    x = np.random.normal(0, 1, size=n_samples)\n",
    "\n",
    "    # Proposal distribution\n",
    "    g_dist = np.random.normal(a, math.sqrt(var1), size=n_samples)\n",
    "    \n",
    "    # Compute importance weights (pdf of normal function) h(x)/g(x)\n",
    "    weights = f(g_dist) / g(g_dist, a, math.sqrt(var1))\n",
    "    \n",
    "   \n",
    "    # p1 = [indicator(i) for i in x]\n",
    "    # p1 = np.mean(p1)\n",
    "\n",
    "    estimate = np.mean(weights * h(g_dist))\n",
    "    variance = np.var(weights * h(g_dist))\n",
    "    \n",
    "    \n",
    "    return estimate, variance\n",
    "\n",
    "a_list = [2, 4]\n",
    "sample_sizes = [10**3, 10**4, 10**5]\n",
    "vars = [1, 2, 3]\n",
    "\n",
    "# Print loop\n",
    "for v in vars:\n",
    "    for a in a_list:\n",
    "        print(f\"Variance = {v}\")\n",
    "        print(f\"  a = {a}\")\n",
    "        for n in sample_sizes:\n",
    "            estimate, variance = importance_sampling(a, n, v)\n",
    "            print(f\"    n = {n}: Estimate: {estimate}, Variance: {variance} \")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the results are very similar between the two methods and that the Importance Sampling method is more precise in the estimates where the MC crude method computes an estimate of the probability that is equal to 0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(8) Firstly the optimal value of lambda is obtained by calculating the variance of h(X)f(X)/g(X) and choosing the lambda for which the variance is the lowest. Finally we compute the mean of h(X)f(X)/g(X) using g(x) as the density function of X and the optimal value for lambda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal lambda value: 1.3000000000000003\n",
      "Estimate: 1.721406740853014, Variance: 3.123076250684738\n",
      "Reference: 1.718281828459045\n"
     ]
    }
   ],
   "source": [
    "#actual distribution for the parameter we estimate\n",
    "def h(x):\n",
    "    return np.exp(x)\n",
    "\n",
    "# pdf of proposed x distribution\n",
    "def g(x, l):\n",
    "    return l * np.exp(-l * x)\n",
    "\n",
    "# pdf of right x distribution\n",
    "#x is uniform and between 0 and 1, so pdf is equal to each generated number\n",
    "def f(x):\n",
    "    return  x\n",
    "\n",
    "N = 10000\n",
    "\n",
    "\n",
    "def indicator_function1(arr, a, b):\n",
    "    return [1 if x >= a and x <= b else 0 for x in arr]\n",
    "\n",
    "def optimal_lambda(N):\n",
    "    variances = []\n",
    "\n",
    "# check the optimal lambda value for the variance that is between 0.1 and 5 in steps of 0.1\n",
    "    for l in np.arange(0.1, 5, 0.1):\n",
    "        \n",
    "        # distribution with pdf = g(x)\n",
    "        x = np.random.exponential(1/l, size=N)\n",
    "\n",
    "        # get random variables that are between 0 and 1\n",
    "        p = indicator_function1(f(x), 0, 1)\n",
    "\n",
    "        weights = p / g(x, l)\n",
    "        est = weights * h(x)\n",
    "        variance = np.var(est, ddof=1)\n",
    "            \n",
    "        variances.append(np.mean(variance))\n",
    "    return np.argmin(variances)\n",
    "\n",
    "\n",
    "optimal_lambda_val = [i for i in np.arange(0.1, 5, 0.1)] [optimal_lambda(N)]\n",
    "print(f\"Optimal lambda value: {optimal_lambda_val}\")\n",
    "\n",
    "\n",
    "\n",
    "def monte_carlo (n_samples, l):\n",
    "\n",
    "    # distribution with pdf = g(x)\n",
    "    samples = np.random.exponential(1/l, size=n_samples)\n",
    "\n",
    "    # get random variables that are between 0 and 1\n",
    "    p = indicator_function1(samples, 0, 1)\n",
    "\n",
    "    weights = p / g(samples, l)\n",
    "    est = weights * h(samples)\n",
    "    return np.mean(est), np.var(est)\n",
    "\n",
    "m, v = monte_carlo (N, optimal_lambda_val)\n",
    "print(f\"Estimate: {m}, Variance: {v}\")\n",
    "print(f\"Reference: {np.exp(1) - 1}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the estimate computed is close to the true value of the integral."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(9) It is meaningful to use the first moment distribution as a sampling distribution since it represents the expected value from some probability distribution. Therefore, since the first moment distribution is known, it can be used as a sampling distribution to derive an IS estimator. \n",
    "The purpose of choosing g(x) is mostly for cases when sampling from g(x) is easier than sampling from f(x). It is possible to chose a function g(x) such that the importance sampling would reduce the estimator, however it is not always guaranteed. Generally,  first moment distribution can be used as a sampling distribution to reduce the variance of an estimate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
